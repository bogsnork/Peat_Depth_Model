---
title: "Dartmoor workthrough with geoR"
author: "Christoph Kratz"
date: "24 August 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
par.ori <- par()
```


Code through with an extract of data centred around dartmoor, using geoR vignette and approach from Young et al 2018.

###Packages
```{r}
library(geoR)
library(tidyverse)
library(sf)
library(sp)
library(gstat)
library(raster)
```
##Set variables
```{r}
rundate <- paste0("run_", format(Sys.time(), format = "%Y%m%d-%H%M"))
```



##Prepare data

```{r}
  #Import observations----
data.shp <- sf::st_read("../data/peat_depths_dartmoor/peat_depths_dartmoor.shp")
data <- data.frame(data.shp)
coords <- st_coordinates(data.shp)
proj4 <- "+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs"
data.sp <- sp::SpatialPointsDataFrame(data = data, coords = coords, proj4string = CRS(proj4))

  #import predictors from cropped raster stack----
elev <- raster("../data/predictor_stack.tif", band = 1)
aspect <- raster("../data/predictor_stack.tif", band = 2)
slope <- raster("../data/predictor_stack.tif", band = 3)
  #create raster stack
predictors <- stack(elev, aspect, slope)
names(predictors) <- c("elev", "aspect", "slope")
#plot(predictors)
  #convert to spatial points dataframe
predictors.sp <- rasterToPoints(x = predictors, spatial = TRUE)
crs(predictors.sp) <- crs(data.sp)

#Extract predictor values at observation locations----
  #create dataframe with peat depth as first col
input.data <- data.frame(peat_depth = data$PEAT_DEPTH)
  #add square root transform field
input.data$sqrt_peat_depth <- sqrt(input.data$peat_depth)
  #raster::extract environmental and topographic data for each peat depth measurement
input.data$elev <- raster::extract(
  elev, coordinates(data.sp)[,1:2])
input.data$aspect <- raster::extract(
  aspect, coordinates(data.sp)[,1:2])
input.data$slope <- raster::extract(
  slope, coordinates(data.sp)[,1:2])

#convert to spatial points data frame----
coords <- st_coordinates(data.shp)[,c("X", "Y")]
proj4 <- "+proj=tmerc +lat_0=49 +lon_0=-2 +k=0.9996012717 +x_0=400000 +y_0=-100000 +datum=OSGB36 +units=m +no_defs"

input.data.sp <- sp::SpatialPointsDataFrame(data = input.data, coords = coords, proj4string = CRS(proj4))
#spplot(input.data.sp)
```


####graph observations and environmental values

```{r}
mapdata <- data.frame(input.data.sp@data, input.data.sp@coords)
head(mapdata)
ggplot(mapdata, aes(x = X, y = Y)) +
  geom_point(aes(colour = peat_depth))+
  scale_color_distiller(palette = "Reds", direction = 1)+
  coord_equal()+
  labs(title = "Peat depth spatial distribution")

ggplot(mapdata, aes(x = elev, y = slope)) +
  geom_point(aes(colour = peat_depth))+
  scale_color_distiller(palette = "Reds", direction = 1)+
  labs(title = "Peat depth: elevation by slope")

ggplot(mapdata, aes(x = aspect, y = peat_depth)) +
  geom_point(aes(colour = peat_depth))+
  scale_color_distiller(palette = "Reds", direction = 1)+
  coord_polar() +
  labs(title = "Peat depth by aspect")
```

###Create prediction area and crop input data

```{r}
#select a well sampled area to predict within----
# plot(input.data.sp)
#     #small area: 
#     crop.extent <- extent(c(xmin = 25575, xmax = 260000, ymin = 75000, ymax = 80000))
  #all dartmoor
crop.extent <- extent(predictors.sp)
# plot(crop(input.data.sp, crop.extent))
input.data.cropped.sp <- crop(input.data.sp, crop.extent)
dat <- data.frame(input.data.cropped.sp@data,
                            input.data.cropped.sp@coords) %>%
  rename(x = X, y = Y)
input.data.cropped.map <- dat
predictors.cropped.sp <- crop(predictors.sp, crop.extent)
predictors.cropped.map <- data.frame(predictors.cropped.sp@data, predictors.cropped.sp@coords)
```



```{r}
#graph it
p.theme <- theme(legend.position = "right")
p.elev <- ggplot(predictors.cropped.map, aes(x = x, y = y)) +
  geom_point(aes(colour = elev)) + coord_equal() +
  geom_text(data = input.data.cropped.map, label = "x", size = 0.8)+
  scale_colour_distiller(palette = "Reds") +  
  labs(#title = "Elevation", 
       x = "", y = ""); p.elev + p.theme

p.slo <- ggplot(predictors.cropped.map, aes(x = x, y = y)) +
  geom_point(aes(colour = slope))  + coord_equal()+ 
  geom_text(data = input.data.cropped.map, label = "x", size = 0.8)+
  scale_colour_distiller(palette = "Greens") +  
  labs(#title = "Slope", 
       x = "", y = ""); p.slo + p.theme
# cowplot::plot_grid(p.elev + p.theme, p.asp + p.theme, p.slo + p.theme, axis = "left", nrow = 3, labels = "")
```

###Convert to inputs to geodata format for geoR

```{r}
input.data.to.use <- input.data.cropped.sp #spdf format
input.data.gd <- as.geodata(obj = input.data.to.use, 
                            data.col = 2, #specify sqrt_peat_depth 
                            #data.col = 1, #or specify peat_depth 
                            covar.col = c(3,4,5)) #specify elev, asp and slope as covariates
#remove duplicates 
dupes <- as.numeric(row.names(dup.coords(input.data.gd)))
cat("There are", length(dupes),"duplicate locations.  ")
if(length(dupes) > 0){
  input.data.gd <- as.geodata(obj = input.data.to.use[-dupes,])
  cat(length(dupes), "duplicates have been removed.")}
rm(input.data.to.use)
#in this case chosen to simply remove both instances of a duplicate record.  If duplicate locations are an important part of data then better to use jitterDupCoords
summary(input.data.gd)
summary(input.data.cropped.sp)
```


## 2. Descriptive plots

```{r}
plot(input.data.gd)
```

```{r}
max_dist <- 4000
#I don't understand these plots
par(mfrow = c(2,2), mar=c(3,3,1,1), mgp=c(2,0.8,0))
points(input.data.gd, xlab = "Coord X", ylab = "Coord Y", main = "peat depth overview")
points(input.data.gd, xlab = "Coord X", ylab = "Coord Y", pt.divide = "rank.prop", main = "rank proportion")
points(input.data.gd, xlab = "Coord X", ylab = "Coord Y", cex.max = 1.7, col = gray(seq(1, 0.1, l=max_dist)), 
       pt.divide = "equal", main = "not")
#points(input.data.gd, pt.divide = "quintile", xlab = "Coord X", ylab = "Coord Y", main = "quintile")
par(par.ori)
##dev.off()

```

```{r}
dat <- data.frame(input.data.cropped.sp@data,
                            input.data.cropped.sp@coords) %>%
  rename(x = X, y = Y)
pairs(dat)
```


NOTE: aspect is circular need to account for this - sine or cosine or arc sine
transformation of it. - IGNORING ASPECT FOR THE TIME BEING

### Exploratory linear model analysis

Using raw peat depth

```{r}
mod.lm.test <- lm(peat_depth ~ elev + slope, data=dat)
summary(mod.lm.test)

resid <- residuals(mod.lm.test)
plot(resid)
qqnorm(resid)
qqline(resid)
dat$resid <- resid
```
not great

Using square root of peat depth:...
```{r}
mod.lm.test <- lm(sqrt_peat_depth ~ elev + slope, data=dat)
summary(mod.lm.test)

resid <- residuals(mod.lm.test)
plot(resid)
qqnorm(resid)
qqline(resid)
dat$resid <- resid
```

better, still not fantastic


## 3. Fit a geostatistical model to the data to look for evidence of spatial autocorrelation after accounting for covariates


 Create a geoR data object - the package that does the spatial modelling
 this requires specifying the response, coordinates and covariates.
 The covariates are based on the model above, if you want different ones you
 will need to change them in covars

```{r}
sp_depth <- as.geodata(obj=dat, coords.col=6:7, data.col=2, covar.col = c(3,5))

# Jitter any duplicated data locations which appear to be identical
names(sp_depth)
#coords, data, covariate (slope, elevation)
#check for duplicate coordinates
dup.coords(sp_depth)
set.seed(2490)
sp_depth <- jitterDupCoords(sp_depth, max=2)
summary(sp_depth)
dup.coords(sp_depth)

```

### Plot the spatial object
In the top left plot the data have been split into 4 colours based on
quartiles (red is highest)

```{r}
plot(sp_depth)
title(rundate)
png(filename = paste0("../outputs/dat_overview_", rundate, ".jpg"))
plot(sp_depth)
title(rundate)
dev.off()
```

### Fit a geostatistical model to all the data

```{r}
sp_depth.v <- variog(sp_depth)
plot(sp_depth.v)
#eyefit(sp_depth.v) #run this in console
```



```{r}
(model.geo.exp <- likfit(geodata=sp_depth, trend = ~sp_depth$covariate$elev+sp_depth$covariate$slope,
                    ini.cov.pars=c(4, 1000), fix.nugget=FALSE,
                    cov.model="exponential"))

(model.geo.sph <- likfit(geodata=sp_depth, trend = ~sp_depth$covariate$elev+sp_depth$covariate$slope,
                    ini.cov.pars=c(4, 1000), fix.nugget=FALSE,
                    cov.model="spherical"))

#chosen model
summary(model.geo.exp)
summary(model.geo.sph)
```



```{r}
plot(sp_depth.v, main = "It's not great...")
lines(model.geo.sph, col = "blue")
lines(model.geo.exp, col = "red")
```


```{r}
#chosen model
model.geo <- likfit(geodata=sp_depth, trend = ~sp_depth$covariate$elev+sp_depth$covariate$slope,
                    ini.cov.pars=c(4, 800), fix.nugget=FALSE,
                    cov.model="spherical")
summary(model.geo)
write_rds(model.geo, path = paste0("../outputs/model.geo_", rundate, ".rds"))
```


```{r}
png(filename = paste0("../outputs/vgm_", rundate, ".jpg"))
plot(sp_depth.v, main = rundate)
lines(model.geo, col = "red")
dev.off()
```


## 4.  Conduct the cross validation exercise

 Split the data into 10 roughly equally sized groups, and use 9 groups to fit
 the model and predict sqrt depth for the tenth group. Repeat this for each of
 the 10 groups in turn.
 This may depend on how the data are split into 10 groups, so repeat this 10
 times with different groupings to average over this variation.

 WARNING - this will take a while (hours) to run for large datasets.

 The code below runs two models:
 
 - a simple linear model with given covariates.
 - a spatial correlation model with the same covariates.
 - common spatial covariance models are exponential, spherical and matern.
 
 This is a non-Bayesian analysis.

#### Create a matrix to store the results
 Store for each model the prediction and a 95% prediction interval
 Fit two models, a linear model with no spatial correlation (LM) and a full
 spatial correlation model with the same mean function (SM).
 LPI and UPI stand for lower / upper prediction interval.


```{r}
## Array - rows, columns, number of arrays
n <- nrow(dat)
results <- array(NA, c(n, 6,10))
colnames(results) <- c("pred_LM", "LPI_LM", "UPI_LM", 
                       "pred_SM", "LPI_SM", "UPI_SM")

#note on results array: the dimensions are: points, metrics, runs

```

Deal with categorical variables
 
```{r}
#uncomment if using categorical variables
# 
# #Which are the columns with factors?
# dat.fact <- dat[ ,sapply(dat, is.factor)]
# #We don't need the Aspect_cat
# dat.fact <- dat.fact[ ,-2]
# #Keep only the unique rows
# dat.fact <- dat.fact[!duplicated(dat.fact), ]
# dummy <- dat[c(1, 2, 18, 106), ]
# dummy <- dat[row.names(dat.fact), ]
# dummy$longitude <- dummy$longitude + rnorm(dim(dummy)[1], sd = 0.01)
# dummy$latitude <- dummy$latitude + rnorm(dim(dummy)[1], sd = 0.01)
# n.dummy <- nrow(dummy)
```

#  
#  
#### The big loop:  
```{r include=FALSE}

# Loop over the 10 replications of splitting the data into 10 equally size groups.

nreps <- 10

for(r in 1:nreps) {
  print(r)
# Split the sites into 10 random groups of equal size.
 
  # Create random orderings until each of the 10 groups has at least one Winter Hill soil type
test=1
    while(test==1)
    {
    data.order <- sample(1:n)
    split_m_total <- length(data.order)
    split_m_G2 <- as.integer(split_m_total / 10)
    split_m_size <- length(data.order) %% 10
    # Therefore split_m_size chunks will have size split_m_G2 + 1 and the
    # remaining will have size split_size_G2
    split_m_G1 <- split_m_G2 + 1

    # For all data
    # To allow for changes in number of rows in dat
    split.matrix <-
      data.frame(sites=data.order,
        group = c(kronecker(1:split_m_size, rep(1,split_m_G1)),
                kronecker((split_m_size + 1):10, rep(1,split_m_G2))))
     # here column 1 is the row numbers and column 2 is the group number
     # Check each group has 2 Soil types
    types <- rep(NA,10)
        for (j in 1:10)
        {
        dat.temp <- dat[split.matrix[split.matrix$group == j,1] , ]
        types[j] <- sum(table(dat.temp$Soil) == nrow(dat.temp))
        }
    test <- max(types)
    }

# Undertake the 10-fold cross validation
   for (i in 1:10) {
     # Set up the fitting and the prediction data sets
    dat.fit <- dat[split.matrix[split.matrix$group!=i,1] , ]
    dat.pred <- dat[split.matrix[split.matrix$group==i,1] , ]
    if(exists("dummy")){dat.pred <- rbind(dat.pred, dummy)}

     # Fit the linear model
    model.lm <- lm(sqrt_peat_depth ~ elev + slope, data=dat.fit)
    model.lm.predictions <- 
      predict(object=model.lm, newdata=dat.pred, interval="prediction")

     # Fit the spatial model
       # Set up the spatial data object for the fitting data set
       # Ensure you add in the correct covariates here that you wish to use
    test.sp <-
      as.geodata(obj=dat.fit, coords.col=6:7, data.col=2, covar.col = c(3,5))
                 #coords.col=1:2, data.col=11, covar.col= covars)
     # Jitter anyduplicated data locations which appear to be identical
    test.sp <- jitterDupCoords(test.sp, max=2)


     # Fit the models
    model.sm <-
      likfit(geodata=test.sp, 
             trend = ~test.sp$covariate$elev + test.sp$covariate$slope,
             ini.cov.pars = c(as.numeric(unlist(as.character(
               model.geo$call$ini.cov.pars))[2:3])), 
             fix.nugget = model.geo$call$fix.nugget,
             cov.model = model.geo$call$cov.model)
     # There are alternative spatial correlation models  so you could use these instead of exponential (e.g. spherical)
    print(r) # Monitor progress; each of the 10 r will be run 10(i) times.
    print(i)

     # Do the predictions
    control.sm <-
      krige.control(type.krige="OK", trend.d=~dat.fit$elev + dat.fit$slope, 
                    trend.l=~dat.pred$elev + dat.pred$slope, obj.model=model.sm)

    kriging.sm <-
      krige.conv(geodata=test.sp, locations=dat.pred[ ,1:2], krige=control.sm)

    model.sm.predictions <-
      cbind(kriging.sm$predict,
            kriging.sm$predict - 1.96 * sqrt(kriging.sm$krige.var),
            kriging.sm$predict + 1.96 * sqrt(kriging.sm$krige.var))

     # Save the results but removing the last n.dummy values added to make the
     # kriging function work
    ifelse(exists("n.dummy"), 
           n.preds <- nrow(dat.pred) - n.dummy, 
           n.preds <- nrow(dat.pred))

    for(j in 1:n.preds)
    {
         # Choose the row (site) the  prediction corresponds to.
        which.row <- which(rownames(dat.pred)[j]==rownames(dat))

         # Save the results
        results[which.row, 1:3, r] <- model.lm.predictions[j, ]
        results[which.row, 4:6, r] <- model.sm.predictions[j, ]
    }
  }
}

rundate <- paste0("run_", format(Sys.time(), format = "%Y%m%d-%H%M"))

```


```{r}
str(results)
results[1,,]
row.names(results[1,,])
colnames(results)
```

Teh 


#### Write predictions to dataframe

```{r}
#combine original data and predictions
results.df <- cbind(id = seq(1:nrow(dat)), dat, data.frame(results))
results.df
#make tidy data
results.ty <- gather(results.df, key = key, value = value, pred_LM.1:UPI_SM.10) %>% 
  separate(col = key, into = c("key", "modeltype", "run")) %>% 
  spread(key = key, value = value)
results.ty

```

```{r}
str(results)
class(results)
results[101,,]
results[101,2,]
```


------------------------------------------------------------------------------

## 5. Summarise the results  

### Compute metrics of interest  

```{r}
#### Bias  
# - measures whether the predictions are too large or too small on average
# - want to be as close to zero as possible but due to random variation will not be exactly zero.
# - mean of prediction minus observation (is that not the residual?)

# Linear model
bias.lm <- mean(results[,1, ] - matrix(rep(dat$sqrt_peat_depth,10), nrow=n, ncol=10,
                            byrow=FALSE))
# Spatial model
bias.sm <- mean(results[,4, ] - matrix(rep(dat$sqrt_peat_depth,10), nrow=n, ncol=10,
                          byrow=FALSE))

#### RMSE  
# - measures average difference between the true and predicted values ignoring sign. 
# - want to be as small as possible  

# Linear model
rmse.lm <- sqrt(mean((results[,1, ] - matrix(rep(dat$sqrt_peat_depth,10), nrow=n, ncol=10,
                                  byrow=FALSE))^2)) 
# Spatial model
rmse.sm <- sqrt(mean((results[,4, ] - matrix(rep(dat$sqrt_peat_depth,10), nrow=n, ncol=10,
                                  byrow=FALSE))^2))

#### Coverage 
# - measures the probability that the 95% prediction intervals contain the true value 
# - want to be 0.95

# Linear model
coverage.lm <- mean(matrix(rep(dat$sqrt_peat_depth,10), nrow=n, ncol=10, byrow=FALSE) >
     results[ ,2, ] & matrix(rep(dat$sqrt_peat_depth,10), nrow=n, ncol=10,
                             byrow=FALSE) < results[ ,3, ])
# Spatial model
coverage.sm <- mean(matrix(rep(dat$sqrt_peat_depth,10), nrow=n, ncol=10, byrow=FALSE) >
     results[ ,5, ] & matrix(rep(dat$sqrt_peat_depth,10), nrow=n, ncol=10,
                             byrow=FALSE) < results[ ,6, ])

#### Interval width   
# - the width of the 95% prediction intervals
# - want to be as small as possible provided that the coverage above is around 0.95. If the coverage is much lower than 0.95 then this is meaningless.

# Linear model
interv.lm <- mean(results[ ,3, ] - results[ ,2, ])
# Spatial model
interv.sm <- mean(results[ ,6, ] - results[ ,5, ])


table.metrics <- data.frame(rbind(
  LM = c(bias = bias.lm, RMSE = rmse.lm, coverage = coverage.lm, interval_width = interv.lm), 
  SM = c(bias = bias.sm, RMSE = rmse.sm, coverage = coverage.sm, inverval_width = interv.sm)))

table.metrics
```

### visualise predictions

```{r}
graphdata <- results.ty %>% #filter(modeltype == "LM") %>% 
  mutate(pred_backtr = pred^2) %>% 
  group_by(modeltype, id) %>% 
  summarise(sqrt_peat_depth = mean(sqrt_peat_depth),
            sqrt_pred = mean(pred),
            peat_depth = mean(peat_depth),
            pred_backtr = mean(pred_backtr)) #calculate mean result for each point

graphmetrics <- graphdata %>% 
  group_by(modeltype) %>% 
  summarise(RMSE =      sqrt(mean((peat_depth -      pred_backtr)^2)),
            RMSE_sqrt = sqrt(mean((sqrt_peat_depth - sqrt_pred)^2)),
            cc = cor(peat_depth, (pred_backtr)))



ggplot(graphdata, aes(x = peat_depth, y = pred_backtr)) +
  geom_point(aes(colour = modeltype, shape = modeltype), position = position_dodge(width = 2))+
  geom_abline(slope = 1) +
  coord_equal() +
  geom_text(data = graphmetrics, 
            aes(x = 50, y = c(100,110), colour = modeltype,
                label = paste("RMSE =", round(RMSE, 2),
                              "cm (backtransf.) \nCC =", round(cc, 2))),
            size = 3, show.legend = F) +
  labs(title = paste("Predicted v observed values \n", rundate))

  
ggsave(filename = paste0("../outputs/predvobs_", rundate, ".png"))
```


```{r}
class(model.geo)
summary(model.geo)

```

------------------------------------------------------------------------------  
## 5.5 Summary outputs

### Table 1. Summary statistics for input and prediction data

```{r}
rbind(
dat %>% summarise(dataset = "observations extract",
                  n_points = length(peat_depth), depth_med = median(peat_depth), 
                  depth_min = min(peat_depth), depth_max = max(peat_depth),
                  elev_min = min(elev), elev_max = max(elev),
                  slope_min = min(slope), slope_max = max(slope))
,

predictors.cropped.sp@data %>% 
  summarise(dataset = "predictors",
            n_points = length(elev), depth_med = as.numeric(NA), 
            depth_min = as.numeric(NA), depth_max = as.numeric(NA),
            elev_min = min(elev), elev_max = max(elev),
            slope_min = min(slope), slope_max = max(slope))
)

```

### Table 2. Performance metrics for spatial and linear models from 10-fold cross-validation simulations


```{r}
table.metrics
```

### Table xx. combined metrics for each model run

```{r}
#compile metrics
metrics.combined <- data.frame(
  run = rundate,
  dat = dat %>% summarise(n_points = length(peat_depth), depth_med = median(peat_depth), 
                          depth_min = min(peat_depth), depth_max = max(peat_depth),
                          elev_min = min(elev), elev_max = max(elev),
                          slope_min = min(slope), slope_max = max(slope)), 
  cv = data.frame(bias.lm, rmse.lm, coverage.lm, interv.lm, bias.sm, rmse.sm, coverage.sm, interv.sm),
  prdr = predictors.cropped.sp@data %>% 
    summarise(n_points = length(elev),elev_min = min(elev), elev_max = max(elev),
              slope_min = min(slope), slope_max = max(slope)),
  mod = data.frame(model.geo[c(1, 2, 4, 5, 6)],
                   intercept = model.geo$beta[[1]],
                   covar1 = model.geo$beta[[2]],
                   covar2 = model.geo$beta[[3]], 
                   model.geo[c(11, 12, 15, 17, 18)])
)


#make tidy
metrics.combined <- metrics.combined %>% mutate_all(as.character) %>% gather(key = metric, value = value, -run); metrics.combined 
#add to csv record
ifelse(file.exists("../outputs/metrics_combined.csv"), 
       yes = write_csv(metrics.combined, "../outputs/metrics_combined.csv", col_names = F, append = T), 
       no = write_csv(metrics.combined, "../outputs/metrics_combined.csv", col_names = T, append = F))
```




------------------------------------------------------------------------------  

## 6. Do some plots

##Variograms

#### Plot the semi-variogram to test for the presence of spatial autocorrelation

```{r}
residuals <- residuals(mod.lm.test)
resid.sp <- as.geodata(obj=dat, coords.col=6:7, data.col=8,
                       covar.col= c(3,5))
#resid.sp <- jitterDupCoords(resid.sp, max=0.01)
vari1 <- variog(resid.sp)
vari1.mc <- variog.mc.env(resid.sp, obj.variog=vari1)

plot(vari1, envelope.obj = vari1.mc, xlab="Distance (m)",
     ylab="Estimated semi-variogram", main = "Spatial autocorrelation of residuals")
```

#### Empirical variograms
Empirical variograms are calculated using the function variog. There are options for the classical or modulus estimator. Results can be returned as variogram clouds, binned or smoothed variograms.

```{r}
plot(variog(input.data.gd))
```



```{r}
max_dist <- 400
cloud1 <- variog(input.data.gd, option = "cloud")#, max.dist = max_dist)
cloud2 <- variog(input.data.gd, option = "cloud", estimator.type = "modulus")#, max.dist = max_dist)
bin1 <- variog(input.data.gd)#, uvec=seq(0, max_dist, l=11))
bin2  <- variog(input.data.gd, estimator.type= "modulus")#, uvec=seq(0, max_dist, l=11))

par(mfrow=c(2,2), pch = 1)
plot(cloud1, main = "cloud: classical estimator")
plot(cloud2, main = "cloud: modulus estimator")
plot(bin1, main = "binned: classical estimator")
plot(bin2, main = "binned: modulus estimator")
#par(par.ori)
```
	  


##Compare model outputs

```{r}
#read in metrics from csv
model_metrics <- read_csv("../outputs/metrics_combined.csv")
```


```{r}
model_metrics
unique(model_metrics$metric)
model_metrics_w <- model_metrics %>% spread(key = metric, value = value)
model_metrics_w
```

```{r}
ggplot(model_metrics_w, aes(x = cv.rmse.sm, y = cv.rmse.lm)) +
  geom_point(aes(colour = run)) +
  coord_equal() +
  expand_limits(x = 0, y = 0)


ggplot(model_metrics_w, aes(x = dat.n_points, y = cv.rmse.sm)) +
  geom_point(aes(colour = run))

```

